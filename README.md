 # 概述
 

对 hanlp 1.x 基于词典的分词源码分析学习 . 

hanlp 本身都是 static method, 不是很适合多词典的方式 , 这里稍微改造下.  使用 fastutil 优化下数据结构 .

基本上 trie 树都是非常占据内存的, 但是似乎没有考虑到对 cpu 多级缓存的充分使用. 这里考虑优化下 .

后续考虑废弃掉业务上 对 dict 分词的依赖, 但是目前审核场景 依旧严重依赖词典的分词, 所以还是要维护一份 .